{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Assignment No. 2: Part 2 (Theory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the homework you are to solve several simple theoretical problems related to machine learning algorithms.\n",
    "* For every separate problem you can get only 0 points or maximal points for this problem. There are **NO INTERMEDIATE scores**.\n",
    "* Your solution must me **COMPLETE**, i.e. contain all required formulas/proofs/detailed explanations.\n",
    "* You must write your solution for any problem right after the words **YOUR SOLUTION**. Attaching pictures of your handwriting is allowed, but **highly discouraged**.\n",
    "## $\\LaTeX$ in Jupyter\n",
    "Jupyter has constantly improving $\\LaTeX$ support. Below are the basic methods to\n",
    "write **neat, tidy, and well typeset** equations in your notebooks:\n",
    "* to write an **inline** equation use \n",
    "```markdown\n",
    "$ you latex equation here $\n",
    "```\n",
    "* to write an equation, that is **displayed on a separate line** use \n",
    "```markdown\n",
    "$$ you latex equation here $$\n",
    "```\n",
    "* to write a **block of equations** use \n",
    "```markdown\n",
    "\\begin{align}\n",
    "    left-hand-side\n",
    "        &= right-hand-side on line 1\n",
    "        \\\\\n",
    "        &= right-hand-side on line 2\n",
    "        \\\\\n",
    "        &= right-hand-side on the last line\n",
    "\\end{align}\n",
    "```\n",
    "The **ampersand** (`&`) aligns the equations horizontally and the **double backslash**\n",
    "(`\\\\`) creates a new line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task1. Bayesian methods (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a dataset $D =(X,y) =\\{(x_i,y_i)\\}^m_{i=1}$, $x_i \\in \\mathbb{R}^d$, $y_i\\in\\mathbb{R}$ it is known,that \n",
    "$$y_i = w^T x_i + \\epsilon$$\n",
    "where $\\epsilon \\sim N(0,\\sigma^2)$, $w  \\sim N(0,\\alpha I)$ . Suppose that $X^T X =I$, where $I$ is the identity matrix. Derive MAP estimation for $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{gather}\n",
    "    w^* = \\arg\\max_w\\log \\big[p(y|w)p(w)\\big]\\\\\n",
    "    p(y|w) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\prod_i\\exp\\left[\n",
    "    -\\frac{1}{2}\\left(\\frac{y_i - w^\\top x_i}{\\sigma}\\right)^2\\right]\\\\\n",
    "    p(w) = \\frac{1}{(2\\pi\\alpha^2)^{\\frac{d}{2}}}\\exp\\left(-\\frac{\\|w\\|^2}{2\\alpha^2}\\right)\n",
    "\\end{gather}$$\n",
    "\n",
    "$$\n",
    "    w^* = \\arg\\min_w\\left[\\frac{1}{2\\sigma^2}\\sum_i (y_i - w^\\top x_i)^2\n",
    "    + \\frac{\\|w\\|^2}{2\\alpha^2}\\right]\n",
    "$$\n",
    "\n",
    "In the last expression, the terms independent of $w$ were omitted  \n",
    "Rewrite the sum as inner product\n",
    "\n",
    "$$\n",
    "    w^* = \\arg\\min_w \\left[\\frac{1}{2\\sigma^2}\\|Y - Xw\\|^2 + \\frac{\\|w\\|^2}{2\\alpha^2}\\right]\n",
    "$$\n",
    "\n",
    "Zero gradient is necessary and sufficient optimum condition in the convex problem\n",
    "\n",
    "$$\\begin{gather}\n",
    "    -\\frac{1}{\\sigma^2}X^\\top(Y-Xw) + \\frac{w}{\\alpha^2} = 0\\\\\n",
    "    X^\\top Y = \\left(X^\\top X + \\frac{\\sigma^2}{\\alpha^2}I\\right)w\n",
    "\\end{gather}$$\n",
    "\n",
    "$$\n",
    "    \\left(X^\\top X + \\frac{\\sigma^2}{\\alpha^2}I\\right)^{-1}X^\\top Y = w^*\n",
    "$$\n",
    "\n",
    "Given that $X^\\top X$, we have\n",
    "\n",
    "$$\n",
    "   w^* = \\frac{\\alpha^2}{\\alpha^2 + \\sigma^2}X^\\top Y\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2. Gaussian Processes 1 (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\sigma_n(\\mathbf{x}_*)$ be a predictive variance at point $\\mathbf{x}_*$ of a Gaussian Process $f_n$ with zero mean and covariance $k(\\cdot,\\cdot)$ that was built using first $n$ training points.\n",
    "Prove that for $\\forall \\mathbf{x}_*$ it holds\n",
    "$$\n",
    "    \\sigma_{n}(\\mathbf{x}_*) \\leq \\sigma_{n-1}(\\mathbf{x}_*).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maksim Velikanov's solution\n",
    "From the lecture about GP we have the formula(p. 25):\n",
    "$$\n",
    "\\sigma_n(x_*)^2 = k_{**} - \\mathbf{k}_{*}^{(n) \\, T} \\left[\\mathbf{K}^{(n)} + \\sigma^2 I^{(n)}\\right]^{-1}\\mathbf{k}_*^{(n)}\n",
    "$$\n",
    "Since $k_{**}$ doesn't depend on $n$, we only need to show that\n",
    "$$\n",
    "\\mathbf{k}_{*}^{(n) \\, T} \\left[\\mathbf{K}^{(n)} + \\sigma^2 I^{(n)}\\right]^{-1}\\mathbf{k}_*^{(n)} \\geq \\mathbf{k}_{*}^{(n-1) \\, T} \\left[\\mathbf{K}^{(n-1)} + \\sigma^2 I^{(n-1)}\\right]^{-1}\\mathbf{k}_*^{(n-1)}\n",
    "$$\n",
    "Now let's use matrix inversion formulas for block matrices, which were used in p. 24 of the lecture(implicitly). To prove the required inequality, we will choose blocks corresponding to the the first $n-1$ training points, and the $n$'th point. \n",
    "$$\n",
    "\\mathbf{k}_{*}^{(n)} = \n",
    "\\begin{bmatrix}\n",
    "\\mathbf{k}_{*}^{(n-1)} \\\\\n",
    "k_{*n}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{K}^{(n)} + \\sigma^2 I^{(n)} = \n",
    "\\begin{bmatrix}\n",
    "\\mathbf{K}^{(n-1)} + \\sigma^2 I^{(n-1)} & \\mathbf{k}^{(n-1)}_n \\\\\n",
    "\\mathbf{k}^{(n-1) \\, T}_n & k_{nn} + \\sigma^2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Now we use inversion formulas for this block matrix. For convinience let's denote $\\left[\\mathbf{K}^{(n)} + \\sigma^2 I^{(n)}\\right]^{-1} = \\mathbf{A}^{(n)}$ and $1 \\, / \\, \\left(k_{nn} - \\mathbf{k}^{(n-1) \\, T}_n \\mathbf{A}^{(n-1)} \\mathbf{k}^{(n-1)}_n\\right) = S_n$\n",
    "$$\n",
    "\\left[\\mathbf{K}^{(n)} + \\sigma^2 I^{(n)}\\right]^{-1} \\equiv \\mathbf{A}^{(n)} = \n",
    "\\begin{bmatrix}\n",
    "\\mathbf{A}^{(n-1)} + \\mathbf{A}^{(n-1)}\\mathbf{k}^{(n-1)}_n S_n \\mathbf{k}^{(n-1) \\, T}_n\\mathbf{A}^{(n-1)} & -\\mathbf{A}^{(n-1)}\\mathbf{k}^{(n-1)}_n S_n \\\\\n",
    "- S_n \\mathbf{k}^{(n-1) \\, T}_n\\mathbf{A}^{(n-1)} & S_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Now let's substitute this result\n",
    "$$\n",
    "\\begin{multline}\n",
    "\\mathbf{k}_{*}^{(n) \\, T} \\left[\\mathbf{K}^{(n)} + \\sigma^2 I^{(n)}\\right]^{-1}\\mathbf{k}_*^{(n)} = \n",
    "\\begin{bmatrix}\n",
    "\\mathbf{k}_{*}^{(n-1)} \\\\\n",
    "k_{*n}\n",
    "\\end{bmatrix} ^T\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{A}^{(n-1)} + \\mathbf{A}^{(n-1)}\\mathbf{k}^{(n-1)}_n S_n \\mathbf{k}^{(n-1) \\, T}_n\\mathbf{A}^{(n-1)} & -\\mathbf{A}^{(n-1)}\\mathbf{k}^{(n-1)}_n S_n \\\\\n",
    "- S_n \\mathbf{k}^{(n-1) \\, T}_n\\mathbf{A}^{(n-1)} & S_n\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{k}_{*}^{(n-1)} \\\\\n",
    "k_{*n}\n",
    "\\end{bmatrix} = \\\\\n",
    "= \\mathbf{k}_{*}^{(n-1) \\, T} \\left[\\mathbf{K}^{(n-1)} + \\sigma^2 I^{(n-1)}\\right]^{-1}\\mathbf{k}_*^{(n-1)} + S_n \\left(  \\mathbf{k}_{*}^{(n-1) \\, T} \\mathbf{A}^{(n-1)}\\mathbf{k}^{(n-1)}_n \\mathbf{k}^{(n-1) \\, T}_n\\mathbf{A}^{(n-1)} \\mathbf{k}_{*}^{(n-1)} - \\mathbf{k}_{*}^{(n-1) \\, T} \\mathbf{A}^{(n-1)}\\mathbf{k}^{(n-1)}_n k_{*n} - k_{*n} \\mathbf{k}^{(n-1) \\, T}_n\\mathbf{A}^{(n-1)} \\mathbf{k}_{*}^{(n-1)}+ k_{*n} k_{*n}\\right)\n",
    "\\end{multline}\n",
    "$$\n",
    "We note that $\\mathbf{k}_{*}^{(n-1) \\, T} \\mathbf{A}^{(n-1)}\\mathbf{k}^{(n-1)}_n$ is actually a scalar, and, due to simmetricity of $\\mathbf{A}^{(n-1)}$, we have $\\mathbf{k}_{*}^{(n-1) \\, T} \\mathbf{A}^{(n-1)}\\mathbf{k}^{(n-1)}_n = \\mathbf{k}^{(n-1) \\, T}_n\\mathbf{A}^{(n-1)} \\mathbf{k}_{*}^{(n-1)}$. Using this we rewrite the last formula\n",
    "$$\n",
    "\\mathbf{k}_{*}^{(n) \\, T} \\left[\\mathbf{K}^{(n)} + \\sigma^2 I^{(n)}\\right]^{-1}\\mathbf{k}_*^{(n)} = \\mathbf{k}_{*}^{(n-1) \\, T} \\left[\\mathbf{K}^{(n-1)} + \\sigma^2 I^{(n-1)}\\right]^{-1}\\mathbf{k}_*^{(n-1)} + S_n \\left( \\mathbf{k}_{*}^{(n-1) \\, T} \\mathbf{A}^{(n-1)}\\mathbf{k}^{(n-1)}_n - k_{*n}\\right)^2\n",
    "$$\n",
    "Finally, we note that $S_n = \\frac{\\det\\left[\\mathbf{K}^{(n-1)} + \\sigma^2 I^{(n-1)}\\right]}{\\det\\left[\\mathbf{K}^{(n)} + \\sigma^2 I^{(n)} \\right]} \\geq 0$ as a ratio of determinants of positive definite matrices. Therefore\n",
    "$$\n",
    "\\sigma_{n-1}(x_*)^2 - \\sigma_n(x_*)^2 = S_n \\left( \\mathbf{k}_{*}^{(n-1) \\, T} \\mathbf{A}^{(n-1)}\\mathbf{k}^{(n-1)}_n - k_{*n}\\right)^2 \\geq 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3. Gaussian Processes 2 (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider you have gaussian distribution on $\\mathbb R$ with zero mean and differentiable by arguments covariation funtion $k(x, \\tilde{x})$. Get an expression for the correlation between the implementation of a Gaussian process  $y(x) âˆ¼ GP (0, k(x, x ^{\\prime}))$ and its derivative $\\frac{\\partial y(\\tilde x)}{\\partial \\tilde x}$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero mean GP:\n",
    "$$\\begin{gather}\n",
    "    \\qquad k(x,x') = \\mathbb E\\left[\\big(y(x) - \\mathbb E[y(x)]\\big)\n",
    "    \\big( y(x') - \\mathbb E[y(x')]\\big)\\right] = \\mathbb E[y(x)y(x')]\\\\\n",
    "    \\sigma^2(x) = k(x,x) = \\mathbb E[y^2(x)]\n",
    "\\end{gather}$$\n",
    "\n",
    "By definition, correlation is:\n",
    "$$\n",
    "    \\mathrm{corr}(y, \\partial_{\\tilde x} y) = \\frac{\n",
    "    \\mathbb E[y\\partial_{\\tilde x} y] - \\mathbb E[y]\\mathbb E[\\partial_{\\tilde x} y]}\n",
    "    {\\sqrt{\\mathbb E[y^2] - \\mathbb E[y]^2}\n",
    "    \\sqrt{\\mathbb E[(\\partial_{\\tilde x} y)^2] - \\mathbb E[\\partial_{\\tilde x} y]^2}}\n",
    "$$\n",
    "\n",
    "Gaussian integral converges, hence we can swap differentiation and integration:\n",
    "\n",
    "$$\\begin{gather}\n",
    "    \\mathbb E[\\partial_{\\tilde x} y] = \\partial_{\\tilde x}\\mathbb E[y] = 0\\\\\n",
    "    \\mathbb E[y(x)\\partial_{\\tilde x} y(\\tilde x)]\n",
    "    = \\partial_{\\tilde x}\\mathbb E[y(x)y(\\tilde x)] = \\partial_{\\tilde x}k(x,\\tilde x)\n",
    "\\end{gather}$$\n",
    "\n",
    "In the last equation we used independence of $y(x)$ from $\\tilde x$\n",
    "\n",
    "Represent square derivative as follows:\n",
    "$$\n",
    "    (\\partial_{\\tilde x} y)^2 = \\partial_x y\\;\\big|_{x = \\tilde x} \\cdot\\partial_{\\tilde x} y\n",
    "$$\n",
    "\n",
    "Then,\n",
    "$$\n",
    "    \\mathbb E[(\\partial_{\\tilde x}y)^2] = \\partial_x\\partial_{\\tilde x } k(x,\\tilde x)\\;\\big|_{x=\\tilde x}\n",
    "$$\n",
    "\n",
    "Ultimately, we have:\n",
    "\n",
    "$$\n",
    "    \\mathrm{corr}(x, x') = \\frac{\\partial_{x'}k(x,x')}{\\sqrt{k(x,x)\\cdot\\partial_x\\partial_{x'}k(x, x')\\;\\big|_{x=x'}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4. Kernel theory (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $K(x, x'):\\mathcal{X}\\times \\mathcal{X}\\rightarrow \\mathbb{R}$ be a PDS kernel,\n",
    "and $\\phi\\colon \\mathcal{X} \\to \\mathcal{H}$ its <b>unknown </b> feature mapping. For $x,x'\\in\\mathcal{X}$ derive the formula for the **distance** between $\n",
    "\\phi(x)$ and $\\phi(x')$ in $\\mathcal{H}$.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $K(x,x')$ is a p.d. kernel, iff $K(x,x') = \\langle\\phi(x),\\phi(x')\\rangle$.  \n",
    "Here $\\phi:\\mathcal X\\to\\mathcal H$ is a mapping from original feature space to some Hilbert space.  \n",
    "\n",
    "Define distance via inner product \n",
    "$d(x, x')\\triangleq\\langle\\phi(x)-\\phi(x'),\\phi(x)-\\phi(x')\\rangle^\\frac{1}{2}$\n",
    "\n",
    "Then,\n",
    "$$\n",
    "    d(x,x') = \\left(\\langle\\phi(x),\\phi(x)\\rangle\n",
    "    - 2\\langle\\phi(x),\\phi(x')\\rangle\n",
    "    + \\langle\\phi(x'),\\phi(x')\\rangle\\right)^\\frac{1}{2}\n",
    "$$\n",
    "\n",
    "And\n",
    "$$\n",
    "    d(x,x') = \\sqrt{K(x,x) - 2K(x,x') + K(x',x')\\rangle}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5. Naive Gradient Boosting Regression (1 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are given a regression dataset, consisting of 5 samples with 1-dimensional feature vector $X$ and scalar target vector $y \\in \\mathbb{R}$:\n",
    "\n",
    "|  x   |  y   | \n",
    "|:----:|:----:| \n",
    "|  10  |  1   | \n",
    "|  32  |  9   | \n",
    "|  46  |  13  | \n",
    "|  54  |  16  | \n",
    "|  63  |  23  | \n",
    "\n",
    "In this task you are asked to implement **3 steps of Gradient Boosting Regression** with decision tree stumps as the learners $h_0, h_1, h_2$. \n",
    "\n",
    "In order to complete this task:\n",
    "1. Refer to the slides on naive boosting for regression in **Lecture 8**.\n",
    "2. Assume that the initial model $f_0$ is the mean of the target vector $y$\n",
    "3. According to the algorithm on the boosting approach for regression from **1.**, compute the residuals\n",
    "4. Manually, find a suitable split among the $x_i$ for each decision tree weak model $h_t(X)$, which minimizes the loss function:\n",
    "\n",
    "$$L_{\\text{split_i}} = \\frac{\\text{Var}_{left\\_split}*N_{1} + \\text{Var}_{right\\_split}*N_{2}}{N_{1}+N_{2}}$$\n",
    "\n",
    "where  $\\text{Var}$ is the variance of the values contained in each leaf, $N_1$ is the number of target values $y$ in the left leaf, $N_{2}$ - in the right leaf\n",
    "\n",
    "5. Perform the Gradient Boosting step on the ensemble model $f_t$ with the resulting decision tree stump predictions (assume that the learning rate $lr=1.0$).\n",
    "\n",
    "**Note on Decision Tree Stumps:** A decision tree stump is a decision tree, which consists only of the root and its immediate leaves. In case of this task, at each iteration you are asked to consider 5 different variants of the decision tree stumps $h_t^i$ - one variant for each of the split candidates $x_i$. You should choose the variant that minimizes the loss written above. The two leaves of the tree are formed according to the rule:\n",
    "\n",
    "```python\n",
    "if x_i < split:\n",
    "    target_value -> left leaf\n",
    "elif x_i >= split:\n",
    "    target_value -> right leaf\n",
    "```\n",
    "**HINT:** Think about what should be `target_value` equal to in case of Gradient Boosting Regression.\n",
    "\n",
    "The prediction of decision tree stump $h_t(x_i)$ is the mean of the values of the according leaf.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The task**:\n",
    "\n",
    "* Fill in the table - round the values of table up to the second digit after decimal point:\n",
    "\n",
    "\n",
    "|   x  |   y  |$f_0$|$$y - f_0$$|$L$|$h_0$|$f_1$|$$y-f_1$$|$L$|$h_1$|$f_2$|$$y - f_2$$|$L$|$h_2$|$F_3$|\n",
    "|------|------|-----|-----------|---|-----|-----|---------|---|-----|-----|-----------|---|-----|-----|\n",
    "|  10  |  1   |  0  |    0      | 0 |  0  |  0  |    0    | 0 |  0  |  0  |    0      | 0 |  0  |  0  | \n",
    "|  32  |  9   |  0  |    0      | 0 |  0  |  0  |    0    | 0 |  0  |  0  |    0      | 0 |  0  |  0  |\n",
    "|  46  |  13  |  0  |    0      | 0 |  0  |  0  |    0    | 0 |  0  |  0  |    0      | 0 |  0  |  0  |\n",
    "|  54  |  16  |  0  |    0      | 0 |  0  |  0  |    0    | 0 |  0  |  0  |    0      | 0 |  0  |  0  |\n",
    "|  63  |  23  |  0  |    0      | 0 |  0  |  0  |    0    | 0 |  0  |  0  |    0      | 0 |  0  |  0  |\n",
    "\n",
    "\n",
    "where $L$ is the loss, calculated by the formula for decision tree stumps above, for each of the 5 split variants of the decision tree stump at each iteration\n",
    "* Write down the splits (the feature values) you have found for each of the tree stumps\n",
    "\n",
    "* Insert the predictions of the full ensemble model and the split values, you have achieved after 3 iterations into the plotting cell below (**COPY AND PASTE** the last column from the table above and the splits list to the plotting cell below, instead of **#your solution**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from itertools import groupby\n",
    "from statistics import mean, pvariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tree(x,F,stumps):\n",
    "    x_range = np.arange(np.min(x), np.max(x)+1)\n",
    "    x_r = []\n",
    "    f_r = []\n",
    "    stmps = [0] + stumps + [np.inf]\n",
    "    for st in range(1,len(stmps)):\n",
    "        x_r.extend([list(group) for k, group in groupby(x_range, lambda x: x<stmps[st] and x>=stmps[st-1]) if k])\n",
    "        f_r.append([f_i for f_i,x_ii in zip(F,x) if x_ii<stmps[st] and x_ii>=stmps[st-1]])\n",
    "    F_to_plot = []\n",
    "    for ft in range(len(f_r)):\n",
    "        #assert len(f_r) == len(x_r)\n",
    "        if len(f_r[ft]) == 1:\n",
    "            F_to_plot.extend([f_r[ft][0]]*len(x_r[ft]))\n",
    "        elif len(f_r[ft]) > 1:\n",
    "            F_to_plot.extend([mean(f_r[ft])]*len(x_r[ft]))\n",
    "    return x_range, F_to_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maksim Velikanov's Solution\n",
    "(provide you here unchanged, author spelling saved)\n",
    "\n",
    "---\n",
    "Since i am afraid i may misunderstand the way i have to fill the table, this is how i understood it.\n",
    "On t'th iteration:\n",
    "* On $i$'th position of $L$ column i put the loss of $h_t$ if the split is chosen at $x_i$\n",
    "* On $i$'th position of $h_t$ column i put the prediction of the learner $h_t$ with optimal split at point $x_i$\n",
    "* For $y-f_t$ and $f_{t+1}$ columns i use optimized models from $t$'th and $(t+1)$'th iterations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_var(data): #statistics.pvariance doesn't allow empty data\n",
    "    if len(data) > 0:\n",
    "        return pvariance(data)\n",
    "    else:\n",
    "        return 0\n",
    "def my_mean(data): #statistics.mean doesn't allow empty data\n",
    "    if len(data) > 0:\n",
    "        return mean(data)\n",
    "    else:\n",
    "        return 0\n",
    "def make_iteration(target, m):\n",
    "    L = np.zeros(m)\n",
    "    for i in range(m):\n",
    "        L[i] = (my_var(target[:i]) * i + my_var(target[i:]) * (m-i)) / m\n",
    "    i_opt = np.argmin(L)\n",
    "    predictions = np.zeros(m)\n",
    "    for i in range(m):\n",
    "        if i < i_opt:\n",
    "            predictions[i] = my_mean(target[:i_opt])\n",
    "        else:\n",
    "            predictions[i] = my_mean(target[i_opt:])\n",
    "    return L, i_opt, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12.4 12.4 12.4 12.4 12.4]\n",
      "2\n",
      "[-11.4  -3.4   0.6   3.6  10.6]\n",
      "[53.44 20.95 16.93 19.83 25.35]\n",
      "[-7.4  -7.4   4.93  4.93  4.93]\n",
      "[ 5.    5.   17.33 17.33 17.33]\n"
     ]
    }
   ],
   "source": [
    "#cell for first iteration\n",
    "y = np.array([1, 9, 13, 16, 23])\n",
    "f0 = y.mean() * np.ones(5)\n",
    "res0 = y - f0\n",
    "L0, i_opt_0, predictions0 = make_iteration(res0, 5)\n",
    "f1 = f0 + predictions0\n",
    "print(np.around(f0,decimals=2))\n",
    "print(\n",
    "    i_opt_0,np.around(res0,decimals=2),\n",
    "    np.around(L0,decimals=2),\n",
    "    np.around(predictions0,decimals=2), \n",
    "    np.around(f1,decimals=2), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[-4.    4.   -4.33 -1.33  5.67]\n",
      "[16.93 12.93 16.93 13.8   8.91]\n",
      "[-1.42 -1.42 -1.42 -1.42  5.67]\n",
      "[ 3.58  3.58 15.92 15.92 23.  ]\n"
     ]
    }
   ],
   "source": [
    "#cell for second iteration\n",
    "res1 = y - f1\n",
    "L1, i_opt_1, predictions1 = make_iteration(res1, 5)\n",
    "f2 = f1 + predictions1\n",
    "print(\n",
    "    i_opt_1,np.around(res1,decimals=2),\n",
    "    np.around(L1,decimals=2),\n",
    "    np.around(predictions1,decimals=2),\n",
    "    np.around(f2,decimals=2), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[-2.58  5.42 -2.92  0.08  0.  ]\n",
      "[8.91 7.24 7.57 8.9  8.91]\n",
      "[-2.58  0.65  0.65  0.65  0.65]\n",
      "[ 1.    4.23 16.56 16.56 23.65]\n"
     ]
    }
   ],
   "source": [
    "#cell for third iteration\n",
    "res2 = y - f2\n",
    "L2, i_opt_2, predictions2 = make_iteration(res2, 5)\n",
    "f3 = f2 + predictions2\n",
    "print(\n",
    "    i_opt_2,np.around(res2,decimals=2),\n",
    "    np.around(L2,decimals=2),\n",
    "    np.around(predictions2,decimals=2),\n",
    "    np.around(f3,decimals=2), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLOTTING CELL##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 4.23, 4.23, 4.23, 4.23, 4.23, 4.23, 4.23, 4.23, 4.23, 4.23, 4.23, 4.23, 4.23, 4.23, 16.56, 16.56, 16.56, 16.56, 16.56, 16.56, 16.56, 16.56, 16.56, 16.56, 16.56, 16.56, 16.56, 16.56, 16.56, 16.56, 16.56, 23.65]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [10,32,46,54,63]\n",
    "y = [1, 9, 13, 16, 23]\n",
    "\n",
    "#note that the order of F(x_i) should be corresponding to the order of x_i in the table\n",
    "\n",
    "############ INSERT YOUR SOLUTION HERE###############\n",
    "F3 = [1., 4.23, 16.56, 16.56, 23.65]\n",
    "splits = [46, 63, 32]\n",
    "\n",
    "x_range, boosted_F_plot = plot_tree(\n",
    "    x, F3, stumps = list(np.sort(splits)))\n",
    "print(boosted_F_plot)\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,6))\n",
    "ax.scatter(x,y, label = 'original')\n",
    "ax.scatter(x, F3, label = 'predicted')\n",
    "ax.plot(\n",
    "    x_range,boosted_F_plot, '--',\n",
    "    linewidth=2, label = 'composite function')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6. AdaBoost (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the following cases,explain how AdaBoost, as given in **Lecture 7**, will treat a weak hypothesis $h_t$ with weighted error $N_t(h_t , w_t )$. Also, in each case, explain why this behavior takes place.\n",
    "1. $N_t = \\frac{1}{2}$\n",
    "2. $N_t > \\frac{1}{2}$\n",
    "3. $N_t = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The error functional $\\tilde R(f_{t+1}) = \\tilde R(f_t)$ does not change. So, the algorithm is indifferent to adding a new classifier, since it does not improve the total loss anyhow. And $h_t$ won't be added. Especially, if the regularization term (on model complexity) is included.\n",
    "\n",
    "2. Considering such estimator, AdaBoost will be able to decrease total error by maximizing $N_t$. So, $h_t$ will be added, which will lead to deterioration of the current results (worsen ensemble expressiveness)\n",
    "\n",
    "3. This case is similar to the first one in the sense that the error functional does not change. But now $h_t$ is superior to what we have already ensembled. So, AdaBoost is better to take only $h_t$, since it outbids any other hypothesis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
